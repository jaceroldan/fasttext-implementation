{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFTGUTRcNsKg",
    "outputId": "2768e493-d455-4433-8c55-2c17d34a7127"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2DqNcH_7NzZg"
   },
   "outputs": [],
   "source": [
    "# Define vocabulary class\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=1):\n",
    "        self.word2idx = {'<UNK>': 0}  # Add a special token for unknown words\n",
    "        self.idx2word = {0: '<UNK>'}\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        word_counts = Counter(re.findall(r'\\w+', ' '.join(texts).lower()))\n",
    "\n",
    "        idx = 1  # Start indices from 1 since 0 is reserved for <UNK>\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= self.min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Return word index or 0 if word is not found in vocab\n",
    "        # TODO: There must be a smarter way to deal with words not in vocabulary.\n",
    "        # Using the <UNK> token above gets me by for now.\n",
    "        return [self.word2idx.get(word, 0) for word in re.findall(r'\\w+', text.lower())]\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class FastTextVocabulary(Vocabulary):\n",
    "    def __init__(self, min_freq=1, ngram_range=(3, 6)):\n",
    "        super().__init__(min_freq)\n",
    "        self.ngram_range = ngram_range\n",
    "        self.ngram2idx = {}\n",
    "        self.idx2ngram = {}\n",
    "        self.ngram_count = 1 # as with the old version, we start at 1, and leave 0 for <UNK>\n",
    "\n",
    "    def _get_ngrams(self, word):\n",
    "        ngrams = []\n",
    "        word = f'<{word}>'\n",
    "        for n in range(self.ngram_range[0], self.ngram_range[1] + 1):\n",
    "            ngrams.extend([word[i:i+n] for i in range(len(word) - n + 1)])\n",
    "        return ngrams\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        super().build_vocab(texts)\n",
    "\n",
    "        for word in self.word2idx:\n",
    "            ngrams = self._get_ngrams(word)\n",
    "            for ngram in ngrams:\n",
    "                if ngram not in self.ngram2idx:\n",
    "                    self.ngram2idx[ngram] = self.ngram_count\n",
    "                    self.idx2ngram[self.ngram_count] = ngram\n",
    "                    self.ngram_count += 1\n",
    "\n",
    "    def encode_word(self, word):\n",
    "        word_idx = self.word2idx.get(word, 0)\n",
    "        ngram_idxs = [self.ngram2idx.get(ng, 0) for ng in self._get_ngrams(word)]\n",
    "        return word_idx, ngram_idxs\n",
    "\n",
    "    def ngram_vocab_size(self):\n",
    "        return len(self.ngram2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u4H9hJyZOKBt"
   },
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "class CustomColeridgeDataset(Dataset):\n",
    "    def __init__(self, csv_file, json_dir, vocab, window_size=2, n_samples=1000, random_state=42):\n",
    "        self.train = pd.read_csv(csv_file)\n",
    "        self.train_items = self.train.sample(n=n_samples, random_state=random_state)\n",
    "        self.json_dir = json_dir\n",
    "        self.vocab = vocab\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        train_id = self.train_items.iloc[idx]['Id']\n",
    "        curr_path = os.path.join(self.json_dir, train_id + '.json')\n",
    "\n",
    "        with open(curr_path, 'r') as file:\n",
    "            curr_json = json.load(file)\n",
    "\n",
    "        text = ''.join([cj['text'] for cj in curr_json])\n",
    "        word_indices = self.vocab.encode(text)\n",
    "\n",
    "        # generate center-context pairs using the window size\n",
    "        center_context_pairs = []\n",
    "        for i, center_word_idx in enumerate(word_indices):\n",
    "            for j in range(max(0, i - self.window_size), min(len(word_indices), i + self.window_size + 1)):\n",
    "                if i != j:\n",
    "                    context_word_idx = word_indices[j]\n",
    "                    center_context_pairs.append((center_word_idx, context_word_idx))\n",
    "\n",
    "        return center_context_pairs\n",
    "\n",
    "\n",
    "\n",
    "class FastTextColeridgeDataset(CustomColeridgeDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        train_id = self.train_items.iloc[idx]['Id']\n",
    "        curr_path = os.path.join(self.json_dir, train_id + '.json')\n",
    "        with open(curr_path, 'r') as file:\n",
    "            curr_json = json.load(file)\n",
    "\n",
    "        text = ''.join([cj['text'] for cj in curr_json])\n",
    "        word_indices = self.vocab.encode(text)\n",
    "\n",
    "        center_context_pairs = []\n",
    "        for i, center_word_idx in enumerate(word_indices):\n",
    "            center_word, center_ngrams = self.vocab.encode_word(center_word_idx)\n",
    "            for j in range(max(0, i - self.window_size), min(len(word_indices), i + self.window_size + 1)):\n",
    "                if i != j:\n",
    "                    context_word, context_ngrams = self.vocab.encode_word(word_indices[j])\n",
    "                    center_context_pairs.append((center_word, center_ngrams, context_word, context_ngrams))\n",
    "\n",
    "        return center_context_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MMeX6PvtOdr_"
   },
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def skipgram_loss(scores, true_labels):\n",
    "    loss = nn.BCEWithLogitsLoss()(scores, true_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "myGfLusWSMNL"
   },
   "outputs": [],
   "source": [
    "# Negative Sampling Loss function\n",
    "def negative_sampling_loss(pos_score, neg_score):\n",
    "    # Labels: 1 for positive samples, 0 for negative samples\n",
    "    pos_labels = torch.ones_like(pos_score)\n",
    "    neg_labels = torch.zeros_like(neg_score)\n",
    "\n",
    "    # Use BCEWithLogitsLoss to calculate loss for both positive and negative pairs\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    pos_loss = bce_loss(pos_score, pos_labels)  # Positive pairs loss\n",
    "    neg_loss = bce_loss(neg_score, neg_labels)  # Negative pairs loss\n",
    "    \n",
    "    # Return the sum of both losses\n",
    "    return pos_loss + neg_loss\n",
    "\n",
    "\n",
    "# Function to generate negative samples\n",
    "def generate_negative_samples(batch_size, vocab_size, num_negative_samples, true_context_word_idxs):\n",
    "    negative_samples = []\n",
    "    for i in range(batch_size):\n",
    "        neg_words = []\n",
    "        while len(neg_words) < num_negative_samples:\n",
    "            neg_sample = random.randint(0, vocab_size - 1)\n",
    "            # Make sure the negative sample is not the true context word\n",
    "            if neg_sample != true_context_word_idxs[i]:\n",
    "                neg_words.append(neg_sample)\n",
    "        negative_samples.append(neg_words)\n",
    "    \n",
    "    return torch.tensor(negative_samples, dtype=torch.long)\n",
    "\n",
    "\n",
    "class FastTextSkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, ngram_vocab_size, embedding_dim, num_negative_samples=5):\n",
    "        super(FastTextSkipGramModel, self).__init__()\n",
    "        self.word_embeddings = nn.Parameter(torch.randn(vocab_size, embedding_dim) * 0.01)\n",
    "        self.ngram_embeddings = nn.Parameter(torch.randn(ngram_vocab_size, embedding_dim) * 0.01)\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "\n",
    "    def forward(self, center_word_idx, center_ngram_idxs, context_word_idx, context_ngram_idxs, negative_word_idxs):\n",
    "        # Positive pair embeddings (center and context)\n",
    "        center_word_embedding = self.word_embeddings[center_word_idx]  # (batch_size, embedding_dim)\n",
    "        context_word_embedding = self.word_embeddings[context_word_idx]  # (batch_size, embedding_dim)\n",
    "\n",
    "        center_ngram_embeddings = self.ngram_embeddings.index_select(0, center_ngram_idxs.view(-1))\n",
    "        center_ngram_embeddings = center_ngram_embeddings.view(center_ngram_idxs.size(0), center_ngram_idxs.size(1), -1)\n",
    "        center_ngram_embeddings = torch.sum(center_ngram_embeddings, dim=1)\n",
    "\n",
    "        context_ngram_embeddings = self.ngram_embeddings.index_select(0, context_ngram_idxs.view(-1))\n",
    "        context_ngram_embeddings = context_ngram_embeddings.view(context_ngram_idxs.size(0), context_ngram_idxs.size(1), -1)\n",
    "        context_ngram_embeddings = torch.sum(context_ngram_embeddings, dim=1)\n",
    "\n",
    "        # Combine word and n-gram embeddings for center and context\n",
    "        center_embedding = center_word_embedding + center_ngram_embeddings\n",
    "        context_embedding = context_word_embedding + context_ngram_embeddings\n",
    "\n",
    "        # Compute positive score (dot product between center and context embeddings)\n",
    "        pos_score = torch.sum(center_embedding * context_embedding, dim=1)  # (batch_size,)\n",
    "\n",
    "        # Negative sampling\n",
    "        neg_word_embeddings = self.word_embeddings[negative_word_idxs]  # (batch_size, num_negative_samples, embedding_dim)\n",
    "        neg_score = torch.bmm(neg_word_embeddings, center_embedding.unsqueeze(2)).squeeze(2)  # (batch_size, num_negative_samples)\n",
    "\n",
    "        return pos_score, neg_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K6YZRVoxSU8D"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "vocab = FastTextVocabulary(min_freq=5)\n",
    "\n",
    "# train = pd.read_csv('/content/drive/My Drive/Datasets/Coleridge/datasets/train.csv')\n",
    "train = pd.read_csv('./datasets/train.csv')\n",
    "train_items = train.sample(n=1000, random_state=42)\n",
    "texts = []\n",
    "\n",
    "# for i in range(len(train_items)):\n",
    "#     curr_path = os.path.join(\n",
    "#         os.getcwd(),\n",
    "#         'drive',\n",
    "#         'My Drive',\n",
    "#         'Datasets',\n",
    "#         'Coleridge',\n",
    "#         'datasets',\n",
    "#         'train',\n",
    "#         train_items.iloc[i]['Id'] + '.json')\n",
    "#     with open(curr_path, 'r') as file:\n",
    "#         curr_json = json.load(file)\n",
    "#         texts.append(''.join([cj['text'] for cj in curr_json]))\n",
    "\n",
    "for i in range(len(train_items)):\n",
    "    curr_path = os.path.join(\n",
    "        os.getcwd(),\n",
    "        'datasets',\n",
    "        'train',\n",
    "        train_items.iloc[i]['Id'] + '.json')\n",
    "    with open(curr_path, 'r') as file:\n",
    "        curr_json = json.load(file)\n",
    "        texts.append(''.join([cj['text'] for cj in curr_json]))\n",
    "\n",
    "vocab.build_vocab(texts)\n",
    "\n",
    "# dataset = FastTextColeridgeDataset(csv_file='/content/drive/My Drive/Datasets/Coleridge/datasets/train.csv', json_dir='/content/drive/My Drive/Datasets/Coleridge/datasets/train/', vocab=vocab)\n",
    "dataset = FastTextColeridgeDataset(csv_file='./datasets/train.csv', json_dir='./datasets/train/', vocab=vocab)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda x: [item for sublist in x for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQ5ph6XWc8c8",
    "outputId": "452a9cfc-b782-4847-e2bb-71df61ae06e7"
   },
   "outputs": [],
   "source": [
    "\n",
    "vocab_size = vocab.vocab_size()\n",
    "ngram_vocab_size = vocab.ngram_vocab_size()\n",
    "embedding_dim = 100\n",
    "model = FastTextSkipGramModel(vocab_size, ngram_vocab_size, embedding_dim)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "for batch in dataloader:\n",
    "    # Unpack the batch correctly\n",
    "    center_word_idxs, center_ngram_idxs, context_word_idxs, context_ngram_idxs = zip(*batch)\n",
    "    \n",
    "    # Convert word indices to tensors\n",
    "    center_word_idxs = torch.tensor(center_word_idxs, dtype=torch.long)\n",
    "    context_word_idxs = torch.tensor(context_word_idxs, dtype=torch.long)\n",
    "\n",
    "    # Convert n-gram indices to tensors, pad them to the same length, and stack them\n",
    "    center_ngram_idxs = [torch.tensor(ngrams, dtype=torch.long) for ngrams in center_ngram_idxs]\n",
    "    context_ngram_idxs = [torch.tensor(ngrams, dtype=torch.long) for ngrams in context_ngram_idxs]\n",
    "\n",
    "    # Pad the n-gram sequences so they all have the same length\n",
    "    center_ngram_idxs_padded = rnn_utils.pad_sequence(center_ngram_idxs, batch_first=True, padding_value=0)\n",
    "    context_ngram_idxs_padded = rnn_utils.pad_sequence(context_ngram_idxs, batch_first=True, padding_value=0)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Pass padded n-grams to the model\n",
    "    scores = model(center_word_idxs, center_ngram_idxs_padded, context_word_idxs, context_ngram_idxs_padded)\n",
    "\n",
    "    # Compute loss and backpropagate\n",
    "    true_labels = torch.ones_like(scores)\n",
    "    loss = skipgram_loss(scores, true_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
