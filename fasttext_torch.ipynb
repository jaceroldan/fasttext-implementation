{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFTGUTRcNsKg",
        "outputId": "2768e493-d455-4433-8c55-2c17d34a7127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vocabulary class\n",
        "class Vocabulary:\n",
        "    def __init__(self, min_freq=1):\n",
        "        self.word2idx = {'<UNK>': 0}  # Add a special token for unknown words\n",
        "        self.idx2word = {0: '<UNK>'}\n",
        "        self.min_freq = min_freq\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        word_counts = Counter(re.findall(r'\\w+', ' '.join(texts).lower()))\n",
        "\n",
        "        idx = 1  # Start indices from 1 since 0 is reserved for <UNK>\n",
        "        for word, count in word_counts.items():\n",
        "            if count >= self.min_freq:\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Return word index or 0 if word is not found in vocab\n",
        "        # TODO: There must be a smarter way to deal with words not in vocabulary.\n",
        "        # Using the <UNK> token above gets me by for now.\n",
        "        return [self.word2idx.get(word, 0) for word in re.findall(r'\\w+', text.lower())]\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "\n",
        "class FastTextVocabulary(Vocabulary):\n",
        "    def __init__(self, min_freq=1, ngram_range=(3, 6)):\n",
        "        super().__init__(min_freq)\n",
        "        self.ngram_range = ngram_range\n",
        "        self.ngram2idx = {}\n",
        "        self.idx2ngram = {}\n",
        "        self.ngram_count = 1 # as with the old version, we start at 1, and leave 0 for <UNK>\n",
        "\n",
        "    def _get_ngrams(self, word):\n",
        "        ngrams = []\n",
        "        word = f'<{word}>'\n",
        "        for n in range(self.ngram_range[0], self.ngram_range[1] + 1):\n",
        "            ngrams.extend([word[i:i+n] for i in range(len(word) - n + 1)])\n",
        "        return ngrams\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        super().build_vocab(texts)\n",
        "\n",
        "        for word in self.word2idx:\n",
        "            ngrams = self._get_ngrams(word)\n",
        "            for ngram in ngrams:\n",
        "                if ngram not in self.ngram2idx:\n",
        "                    self.ngram2idx[ngram] = self.ngram_count\n",
        "                    self.idx2ngram[self.ngram_count] = ngram\n",
        "                    self.ngram_count += 1\n",
        "\n",
        "    def encode_word(self, word):\n",
        "        word_idx = self.word2idx.get(word, 0)\n",
        "        ngram_idxs = [self.ngram2idx.get(ng, 0) for ng in self._get_ngrams(word)]\n",
        "        return word_idx, ngram_idxs\n",
        "\n",
        "    def ngram_vocab_size(self):\n",
        "        return len(self.ngram2idx)"
      ],
      "metadata": {
        "id": "2DqNcH_7NzZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset\n",
        "class CustomColeridgeDataset(Dataset):\n",
        "    def __init__(self, csv_file, json_dir, vocab, window_size=2, n_samples=1000, random_state=42):\n",
        "        self.train = pd.read_csv(csv_file)\n",
        "        self.train_items = self.train.sample(n=n_samples, random_state=random_state)\n",
        "        self.json_dir = json_dir\n",
        "        self.vocab = vocab\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        train_id = self.train_items.iloc[idx]['Id']\n",
        "        curr_path = os.path.join(self.json_dir, train_id + '.json')\n",
        "\n",
        "        with open(curr_path, 'r') as file:\n",
        "            curr_json = json.load(file)\n",
        "\n",
        "        text = ''.join([cj['text'] for cj in curr_json])\n",
        "        word_indices = self.vocab.encode(text)\n",
        "\n",
        "        # generate center-context pairs using the window size\n",
        "        center_context_pairs = []\n",
        "        for i, center_word_idx in enumerate(word_indices):\n",
        "            for j in range(max(0, i - self.window_size), min(len(word_indices), i + self.window_size + 1)):\n",
        "                if i != j:\n",
        "                    context_word_idx = word_indices[j]\n",
        "                    center_context_pairs.append((center_word_idx, context_word_idx))\n",
        "\n",
        "        return center_context_pairs\n",
        "\n",
        "\n",
        "\n",
        "class FastTextColeridgeDataset(CustomColeridgeDataset):\n",
        "    def __getitem__(self, idx):\n",
        "        train_id = self.train_items.iloc[idx]['Id']\n",
        "        curr_path = os.path.join(self.json_dir, train_id + '.json')\n",
        "        with open(curr_path, 'r') as file:\n",
        "            curr_json = json.load(file)\n",
        "\n",
        "        text = ''.join([cj['text'] for cj in curr_json])\n",
        "        word_indices = self.vocab.encode(text)\n",
        "\n",
        "        center_context_pairs = []\n",
        "        for i, center_word_idx in enumerate(word_indices):\n",
        "            center_word, center_ngrams = self.vocab.encode_word(center_word_idx)\n",
        "            for j in range(max(0, i - self.window_size), min(len(word_indices), i + self.window_size + 1)):\n",
        "                if i != j:\n",
        "                    context_word, context_ngrams = self.vocab.encode_word(word_indices[j])\n",
        "                    center_context_pairs.append((center_word, center_ngrams, context_word, context_ngrams))\n",
        "\n",
        "        return center_context_pairs\n"
      ],
      "metadata": {
        "id": "u4H9hJyZOKBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function\n",
        "def skipgram_loss(scores, true_labels):\n",
        "    loss = nn.BCEWithLogitsLoss()(scores, true_labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "MMeX6PvtOdr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FastTextSkipGramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, ngram_vocab_size, embedding_dim):\n",
        "        super(FastTextSkipGramModel, self).__init__()\n",
        "        self.word_embeddings = nn.Parameter(torch.randn(vocab_size, embedding_dim) * 0.01)\n",
        "        self.ngram_embeddings = nn.Parameter(torch.randn(ngram_vocab_size, embedding_dim) * 0.01)\n",
        "\n",
        "    def forward(self, center_word_idx, center_ngram_idxs, context_word_idx, context_ngram_idxs):\n",
        "        # Get word embeddings for center and context words\n",
        "        center_word_embedding = self.word_embeddings[center_word_idx]  # (batch_size, embedding_dim)\n",
        "        context_word_embedding = self.word_embeddings[context_word_idx]  # (batch_size, embedding_dim)\n",
        "\n",
        "        # Gather n-gram embeddings for center and context words\n",
        "        center_ngram_embeddings = self.ngram_embeddings.index_select(0, center_ngram_idxs.view(-1))\n",
        "        center_ngram_embeddings = center_ngram_embeddings.view(center_ngram_idxs.size(0), center_ngram_idxs.size(1), -1)\n",
        "        center_ngram_embeddings = torch.sum(center_ngram_embeddings, dim=1)\n",
        "\n",
        "        context_ngram_embeddings = self.ngram_embeddings.index_select(0, context_ngram_idxs.view(-1))\n",
        "        context_ngram_embeddings = context_ngram_embeddings.view(context_ngram_idxs.size(0), context_ngram_idxs.size(1), -1)\n",
        "        context_ngram_embeddings = torch.sum(context_ngram_embeddings, dim=1)\n",
        "\n",
        "        # Combine word and n-gram embeddings for center and context\n",
        "        center_embedding = center_word_embedding + center_ngram_embeddings\n",
        "        context_embedding = context_word_embedding + context_ngram_embeddings\n",
        "\n",
        "        # Compute similarity (dot product) between center and context embeddings\n",
        "        score = torch.sum(center_embedding * context_embedding, dim=1)\n",
        "        return score"
      ],
      "metadata": {
        "id": "myGfLusWSMNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "vocab = FastTextVocabulary(min_freq=5)\n",
        "\n",
        "train = pd.read_csv('/content/drive/My Drive/Datasets/Coleridge/datasets/train.csv')\n",
        "train_items = train.sample(n=1000, random_state=42)\n",
        "texts = []\n",
        "\n",
        "for i in range(len(train_items)):\n",
        "    curr_path = os.path.join(\n",
        "        os.getcwd(),\n",
        "        'drive',\n",
        "        'My Drive',\n",
        "        'Datasets',\n",
        "        'Coleridge',\n",
        "        'datasets',\n",
        "        'train',\n",
        "        train_items.iloc[i]['Id'] + '.json')\n",
        "    with open(curr_path, 'r') as file:\n",
        "        curr_json = json.load(file)\n",
        "        texts.append(''.join([cj['text'] for cj in curr_json]))\n",
        "\n",
        "vocab.build_vocab(texts)\n",
        "\n",
        "dataset = FastTextColeridgeDataset(csv_file='/content/drive/My Drive/Datasets/Coleridge/datasets/train.csv', json_dir='/content/drive/My Drive/Datasets/Coleridge/datasets/train/', vocab=vocab)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda x: [item for sublist in x for item in sublist])"
      ],
      "metadata": {
        "id": "K6YZRVoxSU8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab_size = vocab.vocab_size()\n",
        "ngram_vocab_size = vocab.ngram_vocab_size()\n",
        "embedding_dim = 100\n",
        "model = FastTextSkipGramModel(vocab_size, ngram_vocab_size, embedding_dim)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "# Ensure n-gram indices are padded and converted to tensors in the DataLoader or preprocessing step\n",
        "for batch in dataloader:\n",
        "    center_word_idxs, center_ngram_idxs, context_word_idxs, context_ngram_idxs = zip(*batch)\n",
        "\n",
        "    # Convert word indices to tensors\n",
        "    center_word_idxs = torch.tensor(center_word_idxs, dtype=torch.long)\n",
        "    context_word_idxs = torch.tensor(context_word_idxs, dtype=torch.long)\n",
        "\n",
        "    # Convert n-gram indices to tensors and pad them\n",
        "    center_ngram_idxs = [torch.tensor(ngrams, dtype=torch.long) for ngrams in center_ngram_idxs]\n",
        "    context_ngram_idxs = [torch.tensor(ngrams, dtype=torch.long) for ngrams in context_ngram_idxs]\n",
        "\n",
        "    center_ngram_idxs_padded = rnn_utils.pad_sequence(center_ngram_idxs, batch_first=True, padding_value=0)\n",
        "    context_ngram_idxs_padded = rnn_utils.pad_sequence(context_ngram_idxs, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Pass everything as tensors into the model\n",
        "    scores = model(center_word_idxs, center_ngram_idxs_padded, context_word_idxs, context_ngram_idxs_padded)\n",
        "\n",
        "    true_labels = torch.ones_like(scores)\n",
        "    loss = skipgram_loss(scores, true_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ5ph6XWc8c8",
        "outputId": "452a9cfc-b782-4847-e2bb-71df61ae06e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.3760277330875397\n"
          ]
        }
      ]
    }
  ]
}